"""
    celery.beat.scheduler.ha_scheduler
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    This is High Available Celery Beat Scheduler. Multiple Celery Beat can be run  with that Scheduler and
    you won't see multiple same tasks as how many beat you run. If a celery beat instance among you run is down, one among still up an running beats
    will be running tasks as new master.

    Supported Failover Strategies:
        * AMQP

    Redis failover strategy is not implemented yet. It will be added soon.




"""

from celery.beat import Scheduler
from celery.beat.scheduler.failover.failover_strategy_factory import FailoverStrategyFactory
from celery.utils.log import get_logger

# -*- coding: utf-8 -*-


logger = get_logger(__name__)


class HAScheduler(Scheduler):
    def __init__(self, *args, **kwargs):
        super(HAScheduler, self).__init__(*args, **kwargs)
        self.failover_strategy_factory = FailoverStrategyFactory(self.app)
        self.failover_strategy = self.failover_strategy_factory.get_failover_strategy('%s.celery.beat.master' % self.app.name)


    def apply_entry(self, entry, *args, **kwargs):
        """
            This method is used to apply tasks into the queue by celery.So, it is overridden not to let multiple beat instances apply same task.
            Only master one will be allowed to apply the task in the entry into the queue. Master is elected by a failover strategy
            which is generated by the failover strategy factory.
        """
        if not self.failover_strategy or self.failover_strategy.is_master():
            return super(HAScheduler, self).apply_entry(entry, *args, **kwargs)
        else:
            logger.info('Scheduler: Won\'t be applying due task %s (%s). Because this is slave node.', entry.name, entry.task)